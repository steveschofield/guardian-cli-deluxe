# Guardian Environment Variables
# Copy this file to .env and fill in your API keys

# =============================================================================
# OSINT API Keys
# =============================================================================

# GitHub Personal Access Token (optional but recommended)
# Get from: https://github.com/settings/tokens
# Required scopes: public_repo (read-only)
# Increases rate limit from 60/hour to 5000/hour
GITHUB_TOKEN=

# =============================================================================
# AI Provider API Keys
# =============================================================================

# Google Gemini API Key
# Get your API key from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=your_gemini_api_key_here

# Optional: Gemini via Vertex AI / ADC (no API key)
# 1) Run: gcloud auth application-default login
# 2) Set a project id/number (used when ai.vertexai: true)
# GOOGLE_CLOUD_PROJECT=your-gcp-project-id-or-number

# OpenRouter API Key (OpenAI-compatible endpoint aggregator)
# Get your key from: https://openrouter.ai/keys
# OPENROUTER_API_KEY=your_openrouter_api_key_here
#
# Optional (recommended by OpenRouter for analytics/rate-limit attribution):
# OPENROUTER_SITE_URL=https://your-site.example
# OPENROUTER_APP_NAME=guardian-cli

# Hugging Face Serverless Inference API
# Get a token from: https://huggingface.co/settings/tokens
# HF_TOKEN=your_huggingface_token_here

# Optional: LangSmith for tracing (debugging AI agents)
# LANGCHAIN_TRACING_V2=true
# LANGCHAIN_API_KEY=your_langsmith_api_key_here
# LANGCHAIN_PROJECT=guardian-pentest

# Optional: Full LLM request/response logging (writes to reports/llm_io_<session>_<timestamp>.jsonl)
# LSG_LOG_LLM_REQUESTS=1
# LSG_DEBUG_SAVE_OUTPUT=1

# Optional: Custom configuration file path
# GUARDIAN_CONFIG_PATH=/path/to/guardian.yaml

# Optional: Custom reports directory
# GUARDIAN_REPORTS_DIR=/path/to/reports
