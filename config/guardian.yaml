# Guardian Configuration File
# Default settings for AI-powered penetration testing

# AI Configuration
ai:
  # Supported providers: gemini, ollama, openrouter, huggingface
  #provider: openrouter
  #model: "xiaomi/mimo-v2-flash:free"
  #model: "qwen/qwen3-coder:free"
  #model: "allenai/olmo-3.1-32b-think:free"
  #base_url: "https://openrouter.ai/api/v1"
  #
  # Ollama example:
  provider: ollama
  # model: "llama3.1:8b"
  # model: deepseek-r1:8b 
  model: llama3.2:3b
  #model: "DeepHat/DeepHat-V1-7B"
  base_url: "http://192.168.1.69:11434"
  #
  # Gemini (Vertex AI via ADC) example (recommended for higher limits):
  # - Install deps: pip install -U google-genai
  # - Authenticate: gcloud auth application-default login
  # - Configure project id OR project number below (NOT an API key):
  
  # provider: gemini
  # model: "gemini-3-flash-preview"
  # temperature: 0.2
  # vertexai: true
  # project: "project-id-"
  # location: "global"

  # Hugging Face Serverless Inference API example:
  # - Create a token (fine-grained is recommended) and set env HF_TOKEN
  # - Model is a Hub repo id, e.g. meta-llama/Meta-Llama-3-8B-Instruct
  # - Use base_url: https://router.huggingface.co/hf-inference/models
  #
  # Hugging Face Router (OpenAI-compatible) example:
  # - Use base_url: https://router.huggingface.co/v1
  # provider: huggingface
  # model: "deepseek-ai/DeepSeek-R1"
  # deephat is based on Qwen/Qwen2.5
  # model: "Qwen/Qwen2.5-Coder-7B-Instruct"
  # base_url: "https://router.huggingface.co/v1"
  # timeout: 60
  # hf_max_retries: 4
  temperature: 0.2

  max_tokens: 8000
  # Limit total characters sent to the LLM (across system prompt + context + current prompt).
  # Helps avoid provider context limits and reduces latency/cost.
  max_input_chars: 40000
  # Max characters of tool output to include in LLM prompts (larger = more context, more tokens/cost)
  max_tool_output_chars: 20000
  # Optional: desired context window size (only enforced for Ollama via num_ctx; other providers may ignore).
  # context_window: 8192
  # Limit total characters sent to the LLM (across system prompt + context + current prompt).
  # Helps avoid provider context limits and reduces latency/cost.
  timeout: 600

# Penetration Testing Settings
pentest:
  # Safety mode prevents automatic execution of risky commands
  safe_mode: true
  
  # Maximum number of tools to run in parallel
  max_parallel_tools: 3
  
  # Require user confirmation before executing any tool
  require_confirmation: true
  
  # Maximum scan depth for recursive operations
  max_depth: 3
  
  # Request timeout for tools (seconds)
  tool_timeout: 300

# Output Settings
output:
  # Default format: markdown, html, json
  format: markdown
  
  # Save path for reports and outputs
  save_path: ./reports
  
  # Include AI reasoning in reports
  include_reasoning: true
  
  # Verbosity level: quiet, normal, verbose, debug
  verbosity: normal

# Scope Validation
scope:
  # Blacklisted IP ranges (CIDR notation)
  blacklist:
    - 127.0.0.0/8
  
  # Require explicit scope file for scanning
  require_scope_file: false
  
  # Maximum number of targets per scan
  max_targets: 100

# Tool Configuration
tools:
  nmap:
    enabled: true
    # Recon / baseline enumeration
    default_args: "-sV -sC"
    # Vulnerability script sweep (NSE "vuln" category)
    # Example: nmap -sV --script vuln <target>
    vuln_args: "-sV --script vuln"
    timing: T4
    
  httpx:
    enabled: true
    threads: 50
    timeout: 10
    
  subfinder:
    enabled: true
    sources: ["crtsh", "hackertarget"]
    
  nuclei:
    enabled: true
    severity: ["critical", "high", "medium"]
    # Nuclei can take longer than the default workflow timeout depending on template set/target size.
    tool_timeout: 900
    templates_paths:
      - ~/nuclei-templates
      - ~/nuclei-templates-extra/kenzer
      - ~/nuclei-templates-extra/geeknik
      - ~/nuclei-templates-extra/emadshanab
    # Nuclei tags are template-defined (run `nuclei -tgl` to see what's available in your template set).
    # This set aims for broad OWASP-style coverage without running the entire template corpus.
    tags: ["cve", "exposure", "misconfig", "default", "default-login", "rce", "cmdi", "ssrf", "sqli", "xss", "lfi", "redirect"]
    rate_limit: 75

  naabu:
    enabled: true
    rate: 1000
    top_ports: 100
    exclude_cdn: true

  katana:
    enabled: true
    depth: 2
    concurrency: 10

  asnmap:
    enabled: true
    include_org: true

  waybackurls:
    enabled: true

  zap:
    enabled: true
    # Run mode: "docker" recommended, or "local" for a locally-installed ZAP script.
    mode: docker
    docker_image: "ghcr.io/zaproxy/zaproxy:stable"
    # baseline = passive (safer), full = active scan (requires pentest.safe_mode: false)
    scan: baseline
    # Time budget for the scan scripts (minutes)
    max_minutes: 10

# Workflow Settings
workflows:
  # Default workflow timeout (seconds)
  timeout: 3600
  
  # Save intermediate results
  save_intermediate: true
  
  # Resume from checkpoint on failure
  resume_on_failure: true

# Logging
logging:
  # Enable audit logging
  enabled: true
  
  # Log file path
  path: ./logs/guardian.log
  
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: INFO
  
  # Log AI decisions
  log_ai_decisions: true
